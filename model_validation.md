| **Validation Type**                              | **Purpose**                                                               | **Example in Crew Standby Context**                                       | **Which Vintage(s) to Use**                                                     | **Rationale**                                                                         | **Expected Model Behavior / Outcome**                                                                      | **Action / Approach if Test Fails**                                                                                                                                             | **Recommended Features / Additional Features if Test Fails**                                                                         |
| ------------------------------------------------ | ------------------------------------------------------------------------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| **Time-Series / Rolling-Origin CV**              | Evaluate generalization in future periods while preserving temporal order | Train on weeks Jan–Mar, validate on Apr; then train Jan–Apr, validate May | Most recent sequential vintages after training period (last 4–8 weeks)          | Ensures model predicts **future unseen weeks**, respects causal order; avoids leakage | Model should accurately forecast standby needs for upcoming weeks; residuals unbiased, no systematic drift | Check for concept drift; retrain using more recent data; consider adding features capturing operational changes                                                                 | Lagged features (last 7–14 days), rolling averages, EWMA of delays or standby counts, recent crew unavailability, forecasted flights |
| **Seasonal Validation**                          | Test performance across **seasonal demand patterns**                      | Train on Spring/Winter weeks, validate on Summer weeks                    | Vintages corresponding to high-demand season (e.g., summer, holidays)           | Ensures model generalizes across **seasonal peaks and troughs**                       | Model should capture seasonal increases/decreases in standby demand without over/underestimation           | Add seasonality features (week\_of\_year, holiday flags); consider splitting model per season or using season-adjusted features                                                 | Week-of-year, month-of-year, holiday indicator, peak-season flags, school vacation flags, temperature or weather seasonality         |
| **Sensitive / Weather Testing**                  | Test model response to small-to-moderate operational variations           | Increase delays by 10–20%, minor weather disturbances                     | Vintages covering typical operational fluctuations (normal + minor disruptions) | Confirms model reacts proportionally to small input changes                           | Model output should change **proportionally**; no extreme swings for small feature variations              | Apply regularization; smooth sensitive features; check feature engineering; retrain if response is too volatile                                                                 | Weather indices, delay counts, small crew absence counts, short-term lag features, moving averages of flight disruptions             |
| **Stress / Extreme Event Testing**               | Evaluate model under **rare, high-impact events**                         | Historical storm or peak holiday week with mass cancellations             | Vintages corresponding to extreme events (holiday spikes, major storms)         | Operationally critical; ensures model predicts adequately under rare conditions       | Model should **conservatively predict higher standby**; avoid underprediction during extreme demand weeks  | Augment training data with past extreme events; include stress indicators; adjust decision thresholds; scenario-based simulations                                               | Flags for storms, extreme weather, holiday week indicators, peak travel days, unexpected mass cancellations, max delay counts        |
| **Backtesting / Holdout Period**                 | Evaluate realistic forward performance                                    | Hold out last 4–6 weeks of historical data, train on earlier vintages     | Most recent vintages in dataset                                                 | Mimics real operational deployment                                                    | Model should maintain performance as in CV; predictions should align with operational reality              | Retrain including recent vintages; check feature drift; recalibrate predictions if systematic bias appears                                                                      | Recent lagged features, updated forecasts of flights, real-time crew availability, short-term EWMA of delays and absences            |
| **Sensitivity / Feature Impact Testing**         | Check dependence on critical features                                     | Remove `weather_index` or `scheduled_flights`; observe change             | Representative vintages across low, normal, high-demand weeks                   | Confirms model uses meaningful drivers                                                | Model predictions should degrade **reasonably** when key features are removed; no unexpected failure       | Investigate feature importance; correct feature engineering; retrain or add proxy features if necessary                                                                         | Proxy features, interaction terms, alternate sources of weather or flight disruption data, additional operational metrics            |
| **Cross-Season / Cross-Year Validation**         | Ensure model generalizes across years                                     | Train on 2022 vintages, validate on same weeks in 2023                    | Vintages from same calendar weeks across different years                        | Tests robustness to yearly variations                                                 | Model should maintain stable accuracy despite year-to-year demand or weather changes                       | Include multi-year trends; retrain with additional historical data; consider ensemble with recent vintages                                                                      | Year-over-year averages, rolling seasonal averages, year indicator, macro trends like fuel price or traffic growth                   |
| **Calibration / Residual Analysis (Regression)** | Check whether predictions match observed standby counts                   | Plot predicted vs actual standby for week; analyze residuals              | All vintages, ideally grouped by season                                         | Ensures model is **unbiased** and reflects real patterns                              | Residuals centered around zero; predictions follow observed counts across all demand levels                | Apply post-training calibration (e.g., isotonic regression or scaling); investigate feature-target bias; retrain with adjusted labels if systematic under/overprediction exists | Residual-based features, log-transformed targets, predicted vs actual ratios, calibration scaling parameters                         |
